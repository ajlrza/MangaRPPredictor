{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajlrza/MangaRPPredictor/blob/main/Manga_RP_Predictor_FINETUNE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers"
      ],
      "metadata": {
        "id": "WDpVOEHeKWTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "aBz5Yt6gO1pT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for file in os.listdir(manga_dataset_directory):\n",
        "\n",
        "    current_path = os.listdir(\"/kaggle/input/manga-dataset-images/\"+f\"{file}\")\n",
        "    for file in current_path:\n",
        "        current_path = os.listdir(\"/kaggle/input/manga-dataset-images/\"+f\"{file}\")\n",
        "        print(os.listdir(file))"
      ],
      "metadata": {
        "id": "KItoq4RQdao1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "manga_dataset_directory = \"/kaggle/input/manga-dataset-images\"\n",
        "dataset_found = \"\"\n",
        "\n",
        "while (dataset_found != \"/kaggle/input/manga-dataset-images/manga_dataset/train/color\"):\n",
        "    current_path = os.listdir(\"/kaggle/input/manga-dataset-images\")\n",
        "    for file in current_path:\n",
        "      new_path = os.listdir(f\"{current_path}/{file}\")\n",
        "      current_path = new_path\n"
      ],
      "metadata": {
        "id": "S8ww0-cEdjd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "manga_dataset_directory = \"/kaggle/input/manga-dataset-images/manga_dataset/train/color\"\n",
        "dataset_found = \"\"\n",
        "\n",
        "while (dataset_found != manga_dataset_directory):\n",
        "    current_path = \"/kaggle/input/manga-dataset-images/\" + os.listdir(\"/kaggle/input/manga-dataset-images\")[0]\n",
        "    while (dataset_found != manga_dataset_directory):\n",
        "      for file in os.listdir(current_path):\n",
        "        if (type(os.listdir(f\"{current_path}/{file}\")) == list):\n",
        "           for file in os.listdir(f\"{current_path}/{file}\"):\n",
        "               #guess this algo kind of needs some user-influence, like user-input and print each status of working directory\n",
        "        new_path = os.listdir(f\"{current_path}/{file}\")\n",
        "        current_path = new_path\n",
        "    if (current_path == manga_dataset_directory):\n",
        "      dataset_found = current_path\n",
        "    else:\n",
        "      current_path = new_path\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "a7-3zCUYXYyA",
        "outputId": "c2b9711d-e919-43e8-9d91-2710aced6ca3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-245/3742668728.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcurrent_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/kaggle/input/manga-dataset-images/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/manga-dataset-images\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataset_found\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmanga_dataset_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mnew_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{current_path}/{file}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mnew_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local Inference on GPU\n",
        "Model page: https://huggingface.co/google/vit-base-patch16-224\n",
        "\n",
        "‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/google/vit-base-patch16-224)\n",
        "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè"
      ],
      "metadata": {
        "id": "gRtrF09sKWTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"image-classification\", model=\"google/vit-base-patch16-224\")\n",
        "pipe(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/parrots.png\")"
      ],
      "metadata": {
        "id": "FZZxWzldKWTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
        "\n",
        "processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "model = AutoModelForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")"
      ],
      "metadata": {
        "id": "B0rNyb_5KWTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remote Inference via Inference Providers\n",
        "Ensure you have a valid **HF_TOKEN** set in your environment. You can get your token from [your settings page](https://huggingface.co/settings/tokens). Note: running this may incur charges above the free tier.\n",
        "The following Python example shows how to run the model remotely on HF Inference Providers, automatically selecting an available inference provider for you.\n",
        "For more information on how to use the Inference Providers, please refer to our [documentation and guides](https://huggingface.co/docs/inference-providers/en/index)."
      ],
      "metadata": {
        "id": "AMih3pYcKWTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['HF_TOKEN'] = 'YOUR_TOKEN_HERE'"
      ],
      "metadata": {
        "id": "x6sOVroSKWTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "client = InferenceClient(\n",
        "    provider=\"auto\",\n",
        "    api_key=os.environ[\"HF_TOKEN\"],\n",
        ")\n",
        "\n",
        "output = client.image_classification(\"cats.jpg\", model=\"google/vit-base-patch16-224\")"
      ],
      "metadata": {
        "id": "u5w86xnPKWTt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}